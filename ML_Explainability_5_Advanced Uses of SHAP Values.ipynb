{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shap\n",
    "\n",
    "# Environment Set-Up for feedback system.\n",
    "from learntools.core import binder\n",
    "binder.bind(globals())\n",
    "from learntools.ml_explainability.ex5 import *\n",
    "print(\"Setup Complete\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('C:/Users/vikar/Desktop/Viraj Karkar/Technocolab/Machine Learning Explainability/hospital-readmissions/train.csv')\n",
    "y = data.readmitted\n",
    "base_features = ['number_inpatient', 'num_medications', 'number_diagnoses', 'num_lab_procedures', \n",
    "                 'num_procedures', 'time_in_hospital', 'number_outpatient', 'number_emergency', \n",
    "                 'gender_Female', 'payer_code_?', 'medical_specialty_?', 'diag_1_428', 'diag_1_414', \n",
    "                 'diabetesMed_Yes', 'A1Cresult_None']\n",
    "\n",
    "# Some versions of shap package error when mixing bools and numerics\n",
    "X = data[base_features].astype(float)\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# For speed, we will calculate shap values on smaller subset of the validation data\n",
    "small_val_X = val_X.iloc[:150]\n",
    "my_model = RandomForestClassifier(n_estimators=30, random_state=1).fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(my_model)\n",
    "shap_values = explainer.shap_values(small_val_X)\n",
    "\n",
    "shap.summary_plot(shap_values[1], small_val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the range of diag_1_428 is wider, largely due to the few points on the far right.\n",
    "feature_with_bigger_range_of_effects = 'diag_1_428'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2\n",
    "Do you believe the range of effects sizes (distance between smallest effect and largest effect) is a good indication of which feature will have a higher permutation importance? Why or why not?\n",
    "\n",
    "If the range of effect sizes measures something different from permutation importance: which is a better answer for the question \"Which of these two features does the model say is more important for us to understand when discussing readmission risks in the population?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution: No. The width of the effects range is not a reasonable approximation to permutation importance. For that matter, the width of the range doesn't map well to any intuitive sense of \"importance\" because it can be determined by just a few outliers. However if all dots on the graph are widely spread from each other, that is a reasonable indication that permutation importance is high. Because the range of effects is so sensitive to outliers, permutation importance is a better measure of what's generally important to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[1], small_val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set following var to \"diag_1_428\" if changing it to 1 has bigger effect.  Else set it to 'payer_code_?'\n",
    "bigger_effect_when_changed = 'diag_1_428'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4\n",
    "Some features (like number_inpatient) have reasonably clear separation between the blue and pink dots. Other variables like num_lab_procedures have blue and pink dots jumbled together, even though the SHAP values (or impacts on prediction) aren't all 0.\n",
    "\n",
    "What do you think you learn from the fact that num_lab_procedures has blue and pink dots jumbled together?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution: The jumbling suggests that sometimes increasing that feature leads to higher predictions, and other times it leads to a lower prediction. Said another way, both high and low values of the feature can have both positive and negative effects on the prediction. The most likely explanation for this \"jumbling\" of effects is that the variable (in this case num_lab_procedures) has an interaction effect with other variables. For example, there may be some diagnoses for which it is good to have many lab procedures, and other diagnoses where suggests increased risk. We don't yet know what other feature is interacting with num_lab_procedures though we could investigate that with SHAP contribution dependence plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[1], small_val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot('num_lab_procedures', shap_values[1], small_val_X)\n",
    "shap.dependence_plot('num_medications', shap_values[1], small_val_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution: Here is the code:\n",
    "\n",
    "shap.dependence_plot('num_lab_procedures', shap_values[1], small_val_X)\n",
    "shap.dependence_plot('num_medications', shap_values[1], small_val_X)\n",
    ". Loosely speaking, num_lab_procedures looks like a cloud with little disernible pattern. It does not slope steeply up nor down at any point. It's hard to say we've learned much from that plot. At the same time, the values are not all very close to 0. So the model seems to think this is a relevant feature. One potential next step would be to explore more by coloring it with different other features to search for an interaction.\n",
    "\n",
    "On the other hand, num_medications clearly slopes up until a value of about 20, and then it turns back down. Without more medical background, this seems a surprising phenomenon... You could do some exploration to see whether these patients have unusual values for other features too. But a good next step would be to discuss this phenomenon with domain experts (in this case, the doctors)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
